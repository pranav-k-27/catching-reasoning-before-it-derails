import timefrom huggingface_hub import InferenceClientclass StudentModel:    """    High-capability reasoning model.    Responsible ONLY for generating reasoning.    """    def __init__(self, model_name, hf_token):        self.client = InferenceClient(            model=model_name,            token=hf_token        )    def generate_reasoning(self, prompt, max_tokens=500, temperature=0.7):        start = time.time()        response = self.client.chat_completion(            messages=[                {                    "role": "user",                    "content": (                        f"{prompt}\n\n"                        "Provide a step-by-step reasoning segment. "                        "Do NOT provide a final answer."                    )                }            ],            max_tokens=max_tokens,            temperature=temperature        )        latency = time.time() - start        reasoning = response.choices[0].message.content.strip()        return reasoning, latency